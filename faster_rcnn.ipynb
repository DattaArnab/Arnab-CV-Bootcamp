{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9d4c9c7b-534b-4bd9-b7f1-ad5dcb69284b","cell_type":"markdown","source":"# Faster R-CNN in PyTorch (inbuilt torchvision model)\n\nThis notebook fine-tunes torchvision's inbuilt Faster R-CNN (ResNet-50 FPN v2) on the PennFudan Pedestrian dataset.\n\nWhat it does:\n- Downloads and prepares PennFudan (boxes derived from masks)\n- Builds DataLoaders with a detection-friendly collate\n- Uses `torchvision.models.detection.fasterrcnn_resnet50_fpn_v2`\n- Replaces the predictor for 2 classes (background + person)\n- Trains, saves a checkpoint, and runs inference + visualization\n\nNotes:\n- Images are tensors in [0,1]. Faster R-CNN handles normalization/resizing internally.\n- On CPU, keep epochs small; GPU recommended for speed.","metadata":{}},{"id":"5cfe4628-4f51-4bf2-8edc-9528857280b6","cell_type":"code","source":"# Install dependencies if missing\nimport sys, subprocess\n\ndef pip_install(pkgs):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n\ntry:\n    import torch, torchvision  # noqa: F401\nexcept Exception:\n    print(\"Installing torch/torchvision (CPU wheels). For GPU, prefer a runtime with CUDA preinstalled.\")\n    pip_install([\"torch\", \"torchvision\", \"torchaudio\"])  \n\ntry:\n    import matplotlib  # noqa: F401\nexcept Exception:\n    pip_install([\"matplotlib\"])  \n\ntry:\n    import tqdm  # noqa: F401\nexcept Exception:\n    pip_install([\"tqdm\"])  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:53:40.085442Z","iopub.execute_input":"2025-09-24T13:53:40.086016Z","iopub.status.idle":"2025-09-24T13:53:40.090740Z","shell.execute_reply.started":"2025-09-24T13:53:40.085992Z","shell.execute_reply":"2025-09-24T13:53:40.090064Z"}},"outputs":[],"execution_count":2},{"id":"b6d16b2d-5c19-4657-ba55-3c7bb1492f5a","cell_type":"code","source":"import os\nimport time\nimport zipfile\nimport random\nimport urllib.request\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import transforms as T\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom tqdm.auto import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\nif device.type == 'cuda':\n    torch.backends.cudnn.benchmark = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:53:44.854270Z","iopub.execute_input":"2025-09-24T13:53:44.854890Z","iopub.status.idle":"2025-09-24T13:53:45.104832Z","shell.execute_reply.started":"2025-09-24T13:53:44.854868Z","shell.execute_reply":"2025-09-24T13:53:45.104114Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"id":"c13f320b-846f-477e-a2f5-bee70019c8e3","cell_type":"markdown","source":"## Download and prepare PennFudanPed\nWe derive bounding boxes from the instance masks. Single class: person (label=1). Background=0, so `num_classes=2`.","metadata":{}},{"id":"5334f781-fead-42bd-b6ed-b2150bebeeb8","cell_type":"code","source":"DATA_ROOT = Path(\"/kaggle/working\")\nPENNFUDAN_URL = \"https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\"\nPENNFUDAN_DIR = DATA_ROOT / \"PennFudanPed\"\n\ndef download_pennfudan():\n    PENNFUDAN_DIR.parent.mkdir(parents=True, exist_ok=True)\n    if PENNFUDAN_DIR.exists():\n        print(\"PennFudan already present.\")\n        return\n    zip_path = DATA_ROOT / \"PennFudanPed.zip\"\n    if not zip_path.exists():\n        print(\"Downloading PennFudanPed...\")\n        urllib.request.urlretrieve(PENNFUDAN_URL, zip_path)\n    print(\"Extracting...\")\n    with zipfile.ZipFile(zip_path, 'r') as zf:\n        zf.extractall(DATA_ROOT)\n    print(\"Done.\")\n\ndownload_pennfudan()\n\nclass PennFudanDataset(Dataset):\n    def __init__(self, root: Path, train: bool = True, transforms=None):\n        self.root = Path(root)\n        self.imgs = sorted((self.root / \"PNGImages\").glob(\"*.png\"))\n        self.masks = sorted((self.root / \"PedMasks\").glob(\"*.png\"))\n        assert len(self.imgs) == len(self.masks)\n        self.train = train\n        if transforms is None:\n            self.transforms = T.Compose([T.PILToTensor(), T.ConvertImageDtype(torch.float32)])  # [0,1]\n        else:\n            self.transforms = transforms\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def _random_hflip(self, image: torch.Tensor, boxes: torch.Tensor, p: float = 0.5):\n        if not self.train or random.random() > p:\n            return image, boxes\n        _, H, W = image.shape\n        image = torch.flip(image, dims=[2])  # horizontal flip\n        if boxes.numel():\n            x1 = boxes[:, 0].clone()\n            x2 = boxes[:, 2].clone()\n            boxes[:, 0] = W - x2\n            boxes[:, 2] = W - x1\n        return image, boxes\n\n    def __getitem__(self, idx):\n        img_path = self.imgs[idx]\n        mask_path = self.masks[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path)\n        mask_np = np.array(mask)\n        obj_ids = np.unique(mask_np)[1:]  # remove background 0\n\n        boxes = []\n        for oid in obj_ids:\n            pos = np.where(mask_np == oid)\n            if pos[0].size == 0 or pos[1].size == 0:\n                continue\n            y1 = np.min(pos[0]); y2 = np.max(pos[0])\n            x1 = np.min(pos[1]); x2 = np.max(pos[1])\n            if (x2 - x1) >= 4 and (y2 - y1) >= 4:\n                boxes.append([x1, y1, x2, y2])\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)  # single class: person=1\n\n        img_t = self.transforms(img)\n        img_t, boxes = self._random_hflip(img_t, boxes, p=0.5)\n\n        area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) if boxes.numel() else torch.as_tensor([], dtype=torch.float32)\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n\n        target = {\n            'boxes': boxes,\n            'labels': labels,\n            'image_id': torch.tensor([idx], dtype=torch.int64),\n            'area': area,\n            'iscrowd': iscrowd,\n        }\n        return img_t, target, img  # include PIL for visualization later\n\nfull_dataset = PennFudanDataset(PENNFUDAN_DIR, train=True)\nprint(\"Total images:\", len(full_dataset))\n\n# Train/test split\nindices = list(range(len(full_dataset)))\nrandom.shuffle(indices)\nsplit = int(0.8 * len(indices))\ntrain_indices, test_indices = indices[:split], indices[split:]\n\nclass SubsetWrap(Dataset):\n    def __init__(self, base, indices, train=True):\n        self.base = base\n        self.indices = indices\n        self.train = train\n    def __len__(self):\n        return len(self.indices)\n    def __getitem__(self, i):\n        prev = self.base.train\n        self.base.train = self.train\n        out = self.base[self.indices[i]]\n        self.base.train = prev\n        return out\n\ntrain_ds = SubsetWrap(full_dataset, train_indices, train=True)\ntest_ds  = SubsetWrap(full_dataset, test_indices, train=False)\nprint(\"Train:\", len(train_ds), \" Test:\", len(test_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:54:08.304736Z","iopub.execute_input":"2025-09-24T13:54:08.305288Z","iopub.status.idle":"2025-09-24T13:54:09.636903Z","shell.execute_reply.started":"2025-09-24T13:54:08.305264Z","shell.execute_reply":"2025-09-24T13:54:09.636342Z"}},"outputs":[{"name":"stdout","text":"Downloading PennFudanPed...\nExtracting...\nDone.\nTotal images: 170\nTrain: 136  Test: 34\n","output_type":"stream"}],"execution_count":4},{"id":"7e771d2c-9398-4817-81c7-b5f9ccf8d197","cell_type":"markdown","source":"## DataLoaders and collate function\nDetection models need a custom collate that returns lists of images and targets.","metadata":{}},{"id":"843b8202-f6c4-462d-8b88-9e0294680f80","cell_type":"code","source":"def collate_fn(batch):\n    images, targets, pil_imgs = list(zip(*batch))\n    return list(images), list(targets), list(pil_imgs)\n\nbatch_size = 2 if device.type == 'cuda' else 1\nnum_workers = 2 if device.type == 'cuda' else 0\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\ntest_loader  = DataLoader(test_ds,  batch_size=1, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:54:19.677111Z","iopub.execute_input":"2025-09-24T13:54:19.677351Z","iopub.status.idle":"2025-09-24T13:54:19.682249Z","shell.execute_reply.started":"2025-09-24T13:54:19.677335Z","shell.execute_reply":"2025-09-24T13:54:19.681602Z"}},"outputs":[],"execution_count":5},{"id":"7502c177-df60-45bb-88c4-ab9b42b21060","cell_type":"markdown","source":"## Build the inbuilt Faster R-CNN model\n- Start from pretrained weights (v2)\n- Replace the predictor for 2 classes (background + person)\n- Train all parameters (optionally freeze backbone)","metadata":{}},{"id":"a419eb58-96b4-483f-b641-cc4d80610f4e","cell_type":"code","source":"num_classes = 2  # background + person\n\ndef get_model(num_classes: int):\n    try:\n        weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n        model = fasterrcnn_resnet50_fpn_v2(weights=weights)\n    except Exception as e:\n        print(\"Falling back to no pretrained weights due to:\", e)\n        model = fasterrcnn_resnet50_fpn_v2(weights=None)\n    # Replace the classifier for our dataset\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n\nmodel = get_model(num_classes).to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=5e-4)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\nprint(\"Model ready. Trainable parameters:\", sum(p.numel() for p in params))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:54:26.018465Z","iopub.execute_input":"2025-09-24T13:54:26.019146Z","iopub.status.idle":"2025-09-24T13:54:28.051777Z","shell.execute_reply.started":"2025-09-24T13:54:26.019121Z","shell.execute_reply":"2025-09-24T13:54:28.051175Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n100%|██████████| 167M/167M [00:00<00:00, 216MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Model ready. Trainable parameters: 43030809\n","output_type":"stream"}],"execution_count":6},{"id":"a21b1875-236a-4b0a-8775-b77bc5e0ce5d","cell_type":"markdown","source":"## Training loop\nThe inbuilt detection model returns a dict of losses in training mode. We sum and backpropagate.","metadata":{}},{"id":"fdad9025-aa58-49c6-a289-7ab71ef0811a","cell_type":"code","source":"def train_one_epoch(epoch: int):\n    model.train()\n    running_loss = 0.0\n    steps = 0\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for i, (images, targets, _) in pbar:\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        running_loss += losses.item()\n        steps += 1\n        pbar.set_description(f\"Epoch {epoch} | loss {losses.item():.3f}\")\n    return running_loss / max(steps, 1)\n\nEPOCHS = 15  # Increase for better results (e.g., 10–15 on GPU)\nfor epoch in range(1, EPOCHS + 1):\n    t0 = time.time()\n    avg_loss = train_one_epoch(epoch)\n    lr_scheduler.step()\n    print(f\"Epoch {epoch} done in {time.time() - t0:.1f}s | avg loss {avg_loss:.4f}\")\n\n# Save checkpoint\nckpt_path = Path(\"./faster_rcnn_pennfudan.pth\")\ntorch.save({ 'model': model.state_dict(), 'num_classes': num_classes }, ckpt_path)\nprint(\"Saved checkpoint to\", ckpt_path.resolve())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T14:00:59.966026Z","iopub.execute_input":"2025-09-24T14:00:59.966849Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/68 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7155759a70bb4b888fb5986c3a2fec66"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 done in 430.6s | avg loss 0.1088\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/68 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4ea767e181142098407d488aaa0f654"}},"metadata":{}}],"execution_count":null},{"id":"4035fe9e-fb5d-4622-8b30-47f4c5cf48eb","cell_type":"markdown","source":"## Inference and visualization\nThe model performs its own postprocessing and NMS internally. We filter detections by a score threshold for display.","metadata":{}},{"id":"fea86d13-a4e7-4fb9-82e8-fdd646c9cacb","cell_type":"code","source":"@torch.inference_mode()\ndef predict(image_tensor: torch.Tensor):\n    model.eval()\n    outputs = model([image_tensor.to(device)])\n    return outputs[0]\n\ndef visualize_detections(pil_img: Image.Image, pred: Dict[str, Any], score_thresh: float = 0.5):\n    boxes = pred['boxes'].detach().cpu().numpy()\n    scores = pred['scores'].detach().cpu().numpy()\n    labels = pred['labels'].detach().cpu().numpy()\n    keep = scores >= score_thresh\n    boxes = boxes[keep]\n    scores = scores[keep]\n    labels = labels[keep]\n\n    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n    ax.imshow(pil_img)\n    for box, sc, lab in zip(boxes, scores, labels):\n        x1, y1, x2, y2 = box\n        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='lime', facecolor='none')\n        ax.add_patch(rect)\n        ax.text(x1, y1-5, f\"person: {sc:.2f}\", color='yellow', fontsize=10,\n                bbox=dict(facecolor='black', alpha=0.5, pad=1))\n    ax.axis('off')\n    plt.show()\n\n# Run on a few test samples\nfor i in range(min(3, len(test_ds))):\n    img_t, target, pil_img = test_ds[i]\n    pred = predict(img_t)\n    print(f\"Image {i}: {len(pred['boxes'])} raw detections, showing score >= 0.6\")\n    visualize_detections(pil_img, pred, score_thresh=0.6)","metadata":{},"outputs":[],"execution_count":null},{"id":"da871e95-f1e1-4210-a8d1-18a83cc7c6e6","cell_type":"markdown","source":"## Optional: reload checkpoint later\nRun this cell to reload the saved weights.","metadata":{}},{"id":"38b8443d-bd4c-443a-a915-65e5c22dcbfd","cell_type":"code","source":"# Reload model from checkpoint (optional)\ndef load_model(ckpt_path: Path):\n    ckpt = torch.load(ckpt_path, map_location=device)\n    m = get_model(ckpt.get('num_classes', 2)).to(device)\n    m.load_state_dict(ckpt['model'])\n    m.eval()\n    return m\n\n_ = load_model(Path('./faster_rcnn_pennfudan.pth'))  # example usage","metadata":{},"outputs":[],"execution_count":null}]}
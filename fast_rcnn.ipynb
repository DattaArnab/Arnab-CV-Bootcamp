{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fast R-CNN in PyTorch (from scratch)\n\nThis notebook implements a minimal Fast R-CNN training pipeline in PyTorch:\n- Dataset: PennFudan Pedestrian Detection (boxes derived from masks)\n- Region proposals: Selective Search\n- Model: ResNet-50 backbone (conv1..layer3), RoIAlign, 2-FC head, class-specific bbox regression\n- Training: IoU-based RoI sampling, classification + Smooth L1 bbox loss\n\nNotes:\n- This is a teaching/reference implementation, optimized for clarity over performance.\n- Expect modest accuracy with short training; selective search is slow. Proposals are cached per image to speed up subsequent epochs.\n- By default we only train the Fast R-CNN head (backbone is frozen) for quicker convergence on CPU; you can unfreeze for better results if you have a GPU.\n","metadata":{}},{"cell_type":"code","source":"# Install dependencies (run once)\nimport sys, subprocess\n\ndef pip_install(pkgs):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n\ntry:\n    import selectivesearch  # noqa: F401\nexcept Exception:\n    pip_install([\"selectivesearch\"])  # pure-python selective search\n\ntry:\n    import skimage  # noqa: F401\nexcept Exception:\n    pip_install([\"scikit-image\"])  \n\ntry:\n    import cv2  # noqa: F401\nexcept Exception:\n    pip_install([\"opencv-python-headless\"])  \n\ntry:\n    import torch, torchvision  # noqa: F401\nexcept Exception:\n    print(\"PyTorch not found. Attempting to install CPU wheels (you can skip if already available).\")\n    pip_install([\"torch\", \"torchvision\", \"torchaudio\"])  \n\npip_install([\"tqdm\"])  # progress bars\npip_install([\"matplotlib\"])  # visualization\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:30:00.290216Z","iopub.execute_input":"2025-09-24T13:30:00.290721Z","iopub.status.idle":"2025-09-24T13:30:24.356986Z","shell.execute_reply.started":"2025-09-24T13:30:00.290696Z","shell.execute_reply":"2025-09-24T13:30:24.356374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport math\nimport time\nimport zipfile\nimport random\nimport urllib.request\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision.ops import roi_align, nms\nfrom torchvision import transforms as T\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport selectivesearch\nfrom tqdm.auto import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD  = [0.229, 0.224, 0.225]\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:30:32.128714Z","iopub.execute_input":"2025-09-24T13:30:32.129560Z","iopub.status.idle":"2025-09-24T13:30:33.410282Z","shell.execute_reply.started":"2025-09-24T13:30:32.129531Z","shell.execute_reply":"2025-09-24T13:30:33.409610Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Download PennFudanPed dataset and define Dataset class\nWe derive bounding boxes from the instance masks. Single class: person (label=1). Background=0.","metadata":{}},{"cell_type":"code","source":"DATA_ROOT = Path(\"/kaggle/working\")\nPENNFUDAN_URL = \"https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\"\nPENNFUDAN_DIR = DATA_ROOT / \"PennFudanPed\"\n\ndef download_pennfudan():\n    PENNFUDAN_DIR.parent.mkdir(parents=True, exist_ok=True)\n    if PENNFUDAN_DIR.exists():\n        print(\"PennFudan already present.\")\n        return\n    zip_path = DATA_ROOT / \"PennFudanPed.zip\"\n    if not zip_path.exists():\n        print(\"Downloading PennFudanPed...\")\n        urllib.request.urlretrieve(PENNFUDAN_URL, zip_path)\n    print(\"Extracting...\")\n    with zipfile.ZipFile(zip_path, 'r') as zf:\n        zf.extractall(DATA_ROOT)\n    print(\"Done.\")\n\ndownload_pennfudan()\n\nclass PennFudanDataset(Dataset):\n    def __init__(self, root: Path, transforms=None):\n        self.root = Path(root)\n        self.imgs = sorted((self.root / \"PNGImages\").glob(\"*.png\"))\n        self.masks = sorted((self.root / \"PedMasks\").glob(\"*.png\"))\n        assert len(self.imgs) == len(self.masks)\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        img_path = self.imgs[idx]\n        mask_path = self.masks[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path)\n        mask_np = np.array(mask)\n        obj_ids = np.unique(mask_np)\n        obj_ids = obj_ids[1:]  # remove background 0\n\n        boxes = []\n        for oid in obj_ids:\n            pos = np.where(mask_np == oid)\n            if pos[0].size == 0 or pos[1].size == 0:\n                continue\n            y1 = np.min(pos[0])\n            y2 = np.max(pos[0])\n            x1 = np.min(pos[1])\n            x2 = np.max(pos[1])\n            # clip and discard tiny boxes\n            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n            if (x2 - x1) >= 4 and (y2 - y1) >= 4:\n                boxes.append([x1, y1, x2, y2])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)  # single class: person=1\n\n        if self.transforms:\n            img_t = self.transforms(img)\n        else:\n            # default: to tensor + imagenet normalize\n            img_t = T.Compose([\n                T.ToTensor(),\n                T.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n            ])(img)\n\n        target = {\n            'boxes': boxes,\n            'labels': labels,\n            'size': torch.tensor([img.height, img.width], dtype=torch.int64)\n        }\n        return img_t, target, img  # return PIL img for proposals/vis\n\nfull_dataset = PennFudanDataset(PENNFUDAN_DIR)\nprint(\"Total images:\", len(full_dataset))\n\n# Split train/test\nindices = list(range(len(full_dataset)))\nrandom.shuffle(indices)\nsplit = int(0.8 * len(indices))\ntrain_indices, test_indices = indices[:split], indices[split:]\n\nclass SubsetWrap(Dataset):\n    def __init__(self, base, indices):\n        self.base = base\n        self.indices = indices\n    def __len__(self):\n        return len(self.indices)\n    def __getitem__(self, i):\n        return self.base[self.indices[i]]\n\ntrain_ds = SubsetWrap(full_dataset, train_indices)\ntest_ds  = SubsetWrap(full_dataset, test_indices)\nprint(\"Train:\", len(train_ds), \" Test:\", len(test_ds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:31:15.069724Z","iopub.execute_input":"2025-09-24T13:31:15.070124Z","iopub.status.idle":"2025-09-24T13:31:16.741240Z","shell.execute_reply.started":"2025-09-24T13:31:15.070101Z","shell.execute_reply":"2025-09-24T13:31:16.740313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Selective Search proposals\nWe use the Python `selectivesearch` package. Proposals are filtered by size/aspect and deduplicated, then clipped to image boundaries and limited to a maximum count for speed.","metadata":{}},{"cell_type":"code","source":"def generate_ss_proposals(np_img: np.ndarray,\n                          scale: int = 450,\n                          sigma: float = 0.8,\n                          min_size: int = 30,\n                          max_proposals: int = 2000,\n                          min_box_size: int = 16,\n                          max_aspect_ratio: float = 4.0) -> np.ndarray:\n    # np_img: HxWx3 RGB uint8\n    assert np_img.dtype == np.uint8 and np_img.ndim == 3\n    H, W = np_img.shape[:2]\n    _, regions = selectivesearch.selective_search(np_img, scale=scale, sigma=sigma, min_size=min_size)\n    seen = set()\n    props = []\n    for r in regions:\n        x, y, w, h = r['rect']\n        if w <= 0 or h <= 0:\n            continue\n        if w < min_box_size or h < min_box_size:\n            continue\n        ar = max(w/h, h/w)\n        if ar > max_aspect_ratio:\n            continue\n        x1, y1 = x, y\n        x2, y2 = x + w, y + h\n        # clip\n        x1 = max(0, min(x1, W-1))\n        y1 = max(0, min(y1, H-1))\n        x2 = max(1, min(x2, W))\n        y2 = max(1, min(y2, H))\n        if x2 - x1 < min_box_size or y2 - y1 < min_box_size:\n            continue\n        key = (x1, y1, x2, y2)\n        if key in seen:\n            continue\n        seen.add(key)\n        props.append([x1, y1, x2, y2, r.get('size', (x2-x1)*(y2-y1))])\n    # sort by region size desc, take top-k\n    props.sort(key=lambda v: v[4], reverse=True)\n    props = np.array([p[:4] for p in props[:max_proposals]], dtype=np.float32)\n    return props\n\ndef pil_to_uint8_rgb(img: Image.Image) -> np.ndarray:\n    return np.array(img.convert('RGB'), dtype=np.uint8)\n\nproposal_cache: Dict[int, torch.Tensor] = {}\n\ndef get_cached_proposals(idx: int, pil_img: Image.Image) -> torch.Tensor:\n    if idx in proposal_cache:\n        return proposal_cache[idx]\n    np_img = pil_to_uint8_rgb(pil_img)\n    props = generate_ss_proposals(np_img)\n    if props.shape[0] == 0:\n        # Fallback: entire image\n        H, W = np_img.shape[:2]\n        props = np.array([[0,0,W,H]], dtype=np.float32)\n    tprops = torch.from_numpy(props)\n    proposal_cache[idx] = tprops\n    return tprops\n\n# quick smoke test on one image\nimg_t, tgt, pil_img = train_ds[0]\nprops0 = get_cached_proposals(train_indices[0], pil_img)\nprint(\"Proposals example:\", props0.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:31:33.905070Z","iopub.execute_input":"2025-09-24T13:31:33.905881Z","iopub.status.idle":"2025-09-24T13:31:35.699038Z","shell.execute_reply.started":"2025-09-24T13:31:33.905851Z","shell.execute_reply":"2025-09-24T13:31:35.698404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## IoU, bbox encoding/decoding, and RoI sampling utilities\nWe sample a minibatch of RoIs per image, with a fraction of positives (IoU≥0.5) and the rest negatives (0.1≤IoU<0.5). Regression targets use the Fast R-CNN parameterization and are class-specific.","metadata":{}},{"cell_type":"code","source":"def box_area(boxes: torch.Tensor) -> torch.Tensor:\n    # boxes: [N, 4] (x1,y1,x2,y2)\n    return (boxes[:, 2] - boxes[:, 0]).clamp(min=0) * (boxes[:, 3] - boxes[:, 1]).clamp(min=0)\n\ndef box_iou(boxes1: torch.Tensor, boxes2: torch.Tensor) -> torch.Tensor:\n    # returns [N, M]\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n\n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n    wh = (rb - lt).clamp(min=0)\n    inter = wh[:, :, 0] * wh[:, :, 1]\n    union = area1[:, None] + area2 - inter + 1e-6\n    return inter / union\n\ndef to_center_wh(boxes: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    x1, y1, x2, y2 = boxes.unbind(-1)\n    w = (x2 - x1).clamp(min=1e-6)\n    h = (y2 - y1).clamp(min=1e-6)\n    cx = x1 + 0.5 * w\n    cy = y1 + 0.5 * h\n    return cx, cy, w, h\n\ndef encode_boxes(gt: torch.Tensor, proposals: torch.Tensor) -> torch.Tensor:\n    # gt/proposals: [N, 4]\n    gcx, gcy, gw, gh = to_center_wh(gt)\n    pcx, pcy, pw, ph = to_center_wh(proposals)\n    tx = (gcx - pcx) / pw\n    ty = (gcy - pcy) / ph\n    tw = torch.log(gw / pw)\n    th = torch.log(gh / ph)\n    return torch.stack([tx, ty, tw, th], dim=-1)\n\ndef decode_boxes(deltas: torch.Tensor, proposals: torch.Tensor, img_size: Tuple[int,int]) -> torch.Tensor:\n    # deltas: [N, 4], proposals: [N, 4]\n    H, W = img_size\n    pcx, pcy, pw, ph = to_center_wh(proposals)\n    dx, dy, dw, dh = deltas.unbind(-1)\n    cx = dx * pw + pcx\n    cy = dy * ph + pcy\n    w = pw * torch.exp(dw)\n    h = ph * torch.exp(dh)\n    x1 = cx - 0.5 * w\n    y1 = cy - 0.5 * h\n    x2 = cx + 0.5 * w\n    y2 = cy + 0.5 * h\n    boxes = torch.stack([x1, y1, x2, y2], dim=-1)\n    # clip to image\n    boxes[:, 0::2] = boxes[:, 0::2].clamp(0, W)\n    boxes[:, 1::2] = boxes[:, 1::2].clamp(0, H)\n    return boxes\n\ndef sample_rois(proposals: torch.Tensor,\n                gt_boxes: torch.Tensor,\n                gt_labels: torch.Tensor,\n                batch_size: int = 128,\n                fg_fraction: float = 0.25,\n                fg_thresh: float = 0.5,\n                bg_thresh_hi: float = 0.5,\n                bg_thresh_lo: float = 0.1,\n                num_classes: int = 2):\n    # Add GT boxes to proposals to ensure positives\n    all_props = torch.cat([proposals, gt_boxes], dim=0)\n\n    ious = box_iou(all_props, gt_boxes)  # [R, G]\n    max_iou, gt_idx = ious.max(dim=1)\n    labels = gt_labels[gt_idx]\n\n    fg_idxs = torch.nonzero(max_iou >= fg_thresh).squeeze(1)\n    bg_idxs = torch.nonzero((max_iou < bg_thresh_hi) & (max_iou >= bg_thresh_lo)).squeeze(1)\n\n    fg_rois_per_image = int(round(batch_size * fg_fraction))\n    fg_idxs = fg_idxs[torch.randperm(fg_idxs.numel())[:fg_rois_per_image]] if fg_idxs.numel() > 0 else fg_idxs\n    bg_rois_per_image = batch_size - fg_idxs.numel()\n    bg_idxs = bg_idxs[torch.randperm(bg_idxs.numel())[:bg_rois_per_image]] if bg_idxs.numel() > 0 else bg_idxs\n\n    keep = torch.cat([fg_idxs, bg_idxs], dim=0)\n    if keep.numel() == 0:\n        # fallback: take top IoUs\n        keep = torch.topk(max_iou, k=min(batch_size, max_iou.numel())).indices\n    rois = all_props[keep]\n\n    roi_labels = labels[keep].clone()\n    # background label = 0\n    roi_labels[torch.arange(roi_labels.numel()) >= fg_idxs.numel()] = 0\n\n    # bbox targets (class-specific)\n    bbox_targets = torch.zeros((rois.size(0), 4 * num_classes), dtype=torch.float32)\n    # only positives get regression targets\n    pos_mask = roi_labels > 0\n    pos_inds = torch.nonzero(pos_mask).squeeze(1)\n    if pos_inds.numel() > 0:\n        gt_assigned = gt_boxes[gt_idx[keep[pos_inds]]]\n        deltas = encode_boxes(gt_assigned, rois[pos_inds])\n        for i, cls in zip(pos_inds, roi_labels[pos_inds]):\n            cls = int(cls.item())\n            start = 4 * cls\n            bbox_targets[i, start:start+4] = deltas[pos_inds == i][0]\n\n    return rois, roi_labels, bbox_targets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:31:57.431580Z","iopub.execute_input":"2025-09-24T13:31:57.432634Z","iopub.status.idle":"2025-09-24T13:31:57.447961Z","shell.execute_reply.started":"2025-09-24T13:31:57.432599Z","shell.execute_reply":"2025-09-24T13:31:57.447408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fast R-CNN model\n- Backbone: ResNet-50 up to layer3 (stride=16)\n- RoIAlign: 7x7 pooling, spatial_scale=1/16\n- Two 1024-d FC layers + classification and bbox regression heads\n- Bbox regression is class-specific (4 x num_classes)\n\nWe freeze the backbone by default for speed; you can set `train_backbone=True` to fine-tune it as well.","metadata":{}},{"cell_type":"code","source":"class FastRCNN(nn.Module):\n    def __init__(self, num_classes: int = 2, pool_size: int = 7, train_backbone: bool = False):\n        super().__init__()\n        self.num_classes = num_classes\n        self.pool_size = pool_size\n        # Load ResNet-50 backbone\n        try:\n            weights = torchvision.models.ResNet50_Weights.DEFAULT\n            resnet = torchvision.models.resnet50(weights=weights)\n        except Exception:\n            resnet = torchvision.models.resnet50(pretrained=True)\n\n        # Use layers up to layer3 (output channels=1024, stride=16)\n        self.backbone = nn.Sequential(\n            resnet.conv1,\n            resnet.bn1,\n            resnet.relu,\n            resnet.maxpool,\n            resnet.layer1,\n            resnet.layer2,\n            resnet.layer3,\n        )\n        self.backbone_out_channels = 1024\n        if not train_backbone:\n            for p in self.backbone.parameters():\n                p.requires_grad = False\n\n        # Head: 2 FC layers of 1024 dims\n        self.avgpool_out = self.backbone_out_channels * self.pool_size * self.pool_size\n        self.fc1 = nn.Linear(self.avgpool_out, 1024)\n        self.fc2 = nn.Linear(1024, 1024)\n        self.dropout = nn.Dropout(0.5)\n        self.cls_score = nn.Linear(1024, num_classes)\n        self.bbox_pred = nn.Linear(1024, 4 * num_classes)\n\n        # Initialize heads\n        for l in [self.fc1, self.fc2, self.cls_score, self.bbox_pred]:\n            nn.init.normal_(l.weight, std=0.01)\n            nn.init.constant_(l.bias, 0)\n\n        # stride=16 -> spatial_scale = 1/16\n        self.spatial_scale = 1.0 / 16.0\n\n    def forward(self, images: torch.Tensor, rois: List[torch.Tensor]):\n        # images: [N,3,H,W], rois: list of [Ri, 4] in image coords\n        feats = self.backbone(images)\n        # RoIAlign returns [sumR, C, pool, pool]\n        pooled = roi_align(feats, rois, output_size=(self.pool_size, self.pool_size),\n                           spatial_scale=self.spatial_scale, sampling_ratio=2, aligned=True)\n        x = pooled.flatten(start_dim=1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        class_logits = self.cls_score(x)\n        bbox_deltas = self.bbox_pred(x)\n        return class_logits, bbox_deltas\n\ndef fast_rcnn_losses(class_logits: torch.Tensor,\n                     bbox_deltas: torch.Tensor,\n                     labels: torch.Tensor,\n                     bbox_targets: torch.Tensor,\n                     num_classes: int) -> Tuple[torch.Tensor, torch.Tensor]:\n    # classification loss\n    cls_loss = F.cross_entropy(class_logits, labels)\n    # bbox loss: only for positives\n    pos_mask = labels > 0\n    if pos_mask.any():\n        pos_inds = torch.nonzero(pos_mask).squeeze(1)\n        # gather class-specific predictions\n        pred = bbox_deltas[pos_inds]\n        tgt = bbox_targets[pos_inds]\n        # For each ROI i with class c, select slice [4c:4c+4]\n        idx = labels[pos_inds]\n        rows = torch.arange(pred.size(0), device=pred.device)\n        cols = (idx * 4).unsqueeze(1) + torch.arange(4, device=pred.device).unsqueeze(0)\n        pred_sel = pred[rows.unsqueeze(1), cols]\n        tgt_sel = tgt[rows.unsqueeze(1), cols]\n        box_loss = F.smooth_l1_loss(pred_sel, tgt_sel, reduction='mean')\n    else:\n        box_loss = torch.tensor(0.0, device=class_logits.device)\n    return cls_loss, box_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:32:13.008722Z","iopub.execute_input":"2025-09-24T13:32:13.009411Z","iopub.status.idle":"2025-09-24T13:32:13.021811Z","shell.execute_reply.started":"2025-09-24T13:32:13.009348Z","shell.execute_reply":"2025-09-24T13:32:13.020895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train loop\nWe cache proposals per image to avoid re-running Selective Search each epoch. By default, we train only the Fast R-CNN head for a few epochs. Increase epochs/batch size if you have a GPU for better results.","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    # batch of size 1 for simplicity\n    return batch[0]\n\ntrain_loader = DataLoader(train_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\ntest_loader  = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\nnum_classes = 2  # background + person\nmodel = FastRCNN(num_classes=num_classes, pool_size=7, train_backbone=True).to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.003, momentum=0.9, weight_decay=1e-4)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n\ndef train_one_epoch(epoch: int, max_images: int = None):\n    model.train()\n    pbar = tqdm(enumerate(train_loader), total=min(len(train_loader), max_images) if max_images else len(train_loader))\n    total_cls, total_box = 0.0, 0.0\n    count = 0\n    for i, (img_t, target, pil_img) in pbar:\n        if max_images and i >= max_images:\n            break\n        img_t = img_t.to(device).unsqueeze(0)  # [1,3,H,W]\n        gt_boxes = target['boxes'].to(device)\n        gt_labels = target['labels'].to(device)\n        H, W = int(target['size'][0]), int(target['size'][1])\n\n        idx_global = train_indices[i] if i < len(train_indices) else None\n        props = get_cached_proposals(idx_global, pil_img).to(device)\n        # sample RoIs\n        rois, roi_labels, bbox_targets = sample_rois(\n            props, gt_boxes, gt_labels,\n            batch_size=128, fg_fraction=0.25,\n            fg_thresh=0.5, bg_thresh_hi=0.5, bg_thresh_lo=0.1,\n            num_classes=num_classes\n        )\n        # forward\n        class_logits, bbox_deltas = model(img_t, [rois])\n        cls_loss, box_loss = fast_rcnn_losses(class_logits, bbox_deltas, roi_labels.to(device), bbox_targets.to(device), num_classes)\n        loss = cls_loss + box_loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_cls += cls_loss.item()\n        total_box += box_loss.item()\n        count += 1\n        pbar.set_description(f\"Epoch {epoch} | cls {cls_loss.item():.3f} box {box_loss.item():.3f}\")\n    return total_cls / max(count,1), total_box / max(count,1)\n\nEPOCHS = 50  # increase for better results\nfor epoch in range(1, EPOCHS+1):\n    t0 = time.time()\n    avg_cls, avg_box = train_one_epoch(epoch, max_images=None)\n    lr_scheduler.step()\n    print(f\"Epoch {epoch} done in {time.time()-t0:.1f}s | avg cls {avg_cls:.3f} box {avg_box:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:37:28.592290Z","iopub.execute_input":"2025-09-24T13:37:28.593206Z","iopub.status.idle":"2025-09-24T13:45:26.288663Z","shell.execute_reply.started":"2025-09-24T13:37:28.593174Z","shell.execute_reply":"2025-09-24T13:45:26.287731Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference and visualization\nFor a given test image:\n- Generate proposals (cached)\n- Forward through Fast R-CNN head\n- Decode class-specific bbox deltas\n- Score with softmax, per-class NMS\n- Visualize top detections for class `person`","metadata":{}},{"cell_type":"code","source":"@torch.inference_mode()\ndef fast_rcnn_infer_single(img_t: torch.Tensor, pil_img: Image.Image, conf_thresh=0.5, nms_thresh=0.5, max_dets=50):\n    model.eval()\n    img_t = img_t.to(device).unsqueeze(0)\n    H, W = pil_img.height, pil_img.width\n    idx = None  # for cache key not used here\n    props = get_cached_proposals(-1 if idx is None else idx, pil_img).to(device)\n    class_logits, bbox_deltas = model(img_t, [props])\n    probs = F.softmax(class_logits, dim=1)  # [R, C]\n\n    detections = []\n    # For each class > 0\n    for c in range(1, model.num_classes):\n        scores = probs[:, c]\n        keep = scores >= conf_thresh\n        if keep.sum() == 0:\n            continue\n        scores = scores[keep]\n        prop_keep = props[keep]\n        deltas_c = bbox_deltas[keep, 4*c:4*c+4]\n        boxes_c = decode_boxes(deltas_c, prop_keep, (H, W))\n        # NMS per class\n        keep_idx = nms(boxes_c, scores, nms_thresh)\n        boxes_c = boxes_c[keep_idx]\n        scores = scores[keep_idx]\n        for b, s in zip(boxes_c.cpu().numpy(), scores.cpu().numpy()):\n            detections.append((int(c), float(s), b))\n    # sort by score\n    detections.sort(key=lambda x: x[1], reverse=True)\n    return detections[:max_dets]\n\ndef visualize_detections(pil_img: Image.Image, detections, class_names={1: 'person'}):\n    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n    ax.imshow(pil_img)\n    for cls, score, box in detections:\n        x1, y1, x2, y2 = box\n        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='lime', facecolor='none')\n        ax.add_patch(rect)\n        ax.text(x1, y1-5, f\"{class_names.get(cls, str(cls))}: {score:.2f}\", color='yellow', fontsize=10,\n                bbox=dict(facecolor='black', alpha=0.5, pad=1))\n    ax.axis('off')\n    plt.show()\n\n# Run on a few test images\nfor i in range(min(3, len(test_ds))):\n    img_t, target, pil_img = test_ds[i]\n    dets = fast_rcnn_infer_single(img_t, pil_img, conf_thresh=0.5, nms_thresh=0.5)\n    print(f\"Image {i}: {len(dets)} detections\")\n    visualize_detections(pil_img, dets)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:45:39.032564Z","iopub.execute_input":"2025-09-24T13:45:39.032825Z","iopub.status.idle":"2025-09-24T13:45:40.085101Z","shell.execute_reply.started":"2025-09-24T13:45:39.032806Z","shell.execute_reply":"2025-09-24T13:45:40.084202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tips and extensions\n- Increase EPOCHS and lower `conf_thresh` initially to inspect detections early.\n- Unfreeze backbone (`train_backbone=True`) for better performance on GPU.\n- Add bbox target normalization (mean/std) for more stable training.\n- Hard example mining: adjust bg thresholds or sample more RoIs.\n- Multi-class datasets: change labels and `num_classes` accordingly.\n- For speed and accuracy, consider Faster R-CNN (with an RPN) via `torchvision.models.detection.fasterrcnn_resnet50_fpn`.\n","metadata":{}}]}